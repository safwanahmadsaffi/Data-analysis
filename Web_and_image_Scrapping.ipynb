{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **WEB SCRAPPING AND IMAGE SCRAPPING USING PYTHON**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "JtVKsX6-lRH5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. Introduction to Web Scraping**\n",
        "- **What is Web Scraping?**: Explain that web scraping is a way to extract information from websites automatically. Compare it to copying and pasting text from a website, but faster and done by a program.\n",
        "- **Ethical Considerations**: Mention the importance of scraping responsibly and respecting website terms of service.\n",
        "\n",
        "### **2. Setting Up the Environment**\n",
        "- **Python Basics**: If they are new to Python, give a brief introduction. Show them how to install Python and a text editor like VS Code or Jupyter Notebook.\n",
        "- **Installing Libraries**:\n",
        "  - Install the `requests` and `BeautifulSoup` libraries using pip:\n",
        "    ```bash\n",
        "    pip install requests beautifulsoup4\n",
        "    ```\n",
        "\n",
        "### **3. Basic Example: Scraping a Simple Web Page**\n",
        "Start with a basic example like scraping the title of a webpage.\n",
        "\n",
        "- **Explain Each Step**:\n",
        "  - **Step 1**: Explain what a URL is and how `requests.get()` fetches the webpage.\n",
        "  - **Step 2**: Introduce HTML and how `BeautifulSoup` helps in parsing it.\n",
        "  - **Step 3**: Show how to extract specific elements, like the title.\n",
        "#### **Example: Scraping a Web Page Title**\n"
      ],
      "metadata": {
        "id": "Spd_HRkchjqh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11z40X6fM7RE",
        "outputId": "568f9f0e-50e4-42b8-a126-26e1b7220a44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Title of the webpage: Hacker News\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Step 1: Send a request to the website\n",
        "url = \"https://news.ycombinator.com/\"\n",
        "response = requests.get(url)\n",
        "\n",
        "# Step 2: Parse the HTML content\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# Step 3: Extract the title of the webpage\n",
        "title = soup.title.string                   #give about the domain\n",
        "print(\"Title of the webpage:\", title)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **4. Intermediate Example: Scraping Multiple Items**\n",
        "Move on to scraping multiple items, like a list of headlines or links from a news website.\n",
        "\n",
        "\n",
        "### **5. Advanced Example: Scraping with Pagination**\n",
        "Show them how to scrape data from multiple pages, such as all the headlines across multiple pages.\n",
        "\n",
        "#### **Example: Scraping Multiple Pages**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LfZRwAu-iSSM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Function to scrape a single page\n",
        "def scrape_page(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    headlines = soup.find_all('span', class_='titleline')\n",
        "    return [headline.text for headline in headlines]\n",
        "\n",
        "# Scrape headlines from the first 3 pages\n",
        "base_url = \"https://news.ycombinator.com/news?p=\"\n",
        "all_headlines = []\n",
        "\n",
        "for page in range(1, 2):  # Scraping the first 1 pages\n",
        "    page_url = base_url + str(page)\n",
        "    all_headlines.extend(scrape_page(page_url))\n",
        "\n",
        "# Print all headlines\n",
        "for index, headline in enumerate(all_headlines):\n",
        "    print(f\"{index + 1}. {headline}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKLojJCjRdfR",
        "outputId": "3b0a2b9e-99da-4144-ec9d-be8c6e2c9299"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Blitz: A lightweight, modular, extensible web renderer (github.com/dioxuslabs)\n",
            "2. Show HN: OSM – self host the entire planet in ~30 minutes (gist.github.com)\n",
            "3. Isometric Projection in Game Development (pikuma.com)\n",
            "4. Verso – web browser built on top of the Servo web engine (github.com/versotile-org)\n",
            "5. PermitFlow (YC W22) Is Hiring Senior/Staff+ Engineers and Designers in NYC (ashbyhq.com)\n",
            "6. Generating Simpson's Paradox with Z3 (kevinlynagh.com)\n",
            "7. Adbfs-rootless – Mount Android phones on Linux with adb. No root required (github.com/spion)\n",
            "8. Interstellar movie is implemented with Einstein's equations in 40k lines C++ (twitter.com/bitfield)\n",
            "9. It took my savings and 14 years but I’m about to beat arthritis (thetimes.com)\n",
            "10. Show HN: Anycast+ – An AI-powered podcast app (anycast.website)\n",
            "11. Server Mono: A Typeface Inspired by Typewriters, Apple's SF Mono, and CLIs (servermono.com)\n",
            "12. ASCII 3D Renderer for JavaScript (github.com/kciter)\n",
            "13. Things I've learned building a modern TUI Framework (2022) (textualize.io)\n",
            "14. How good can you be at Codenames without knowing any words? (danluu.com)\n",
            "15. OpenBSD 7.5 via QEMU on Hetzner physical machine (no phys. access / KVM console) (gfuzz.de)\n",
            "16. Tree Attention: Topology-Aware Decoding for Long-Context (arxiv.org)\n",
            "17. Genetics solves a thorny problem: how plants have prickles (cosmosmagazine.com)\n",
            "18. Ancient DNA Revolution (archaeology.org)\n",
            "19. How to avoid losing items? Holding pens (alexwendland.com)\n",
            "20. Introduction to Golang Preemption Mechanisms (unskilled.blog)\n",
            "21. Segment Anything Model and Friends (lightly.ai)\n",
            "22. The 1986 Oldsmobile Incas Dashboard (2020) (thedrive.com)\n",
            "23. Comma.ai: Refactoring for Growth (comma.ai)\n",
            "24. KnitScape (washington.edu)\n",
            "25. OpenDevin: An Open Platform for AI Software Developers as Generalist Agents (arxiv.org)\n",
            "26. Show HN: Simple Science – The Newest Science Explained Simply (simplescience.ai)\n",
            "27. Show HN: Pixeltune, a nicer chiptune and VGM player (pixeltune.org)\n",
            "28. Kitchen sponges can be used as memory devices (nature.com)\n",
            "29. Finite State Machine Designer (madebyevan.com)\n",
            "30. Show HN: Copy-Paste from Webflow into Webstudio (webstudio.is)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certainly! Let's go through the code line by line:\n",
        "\n",
        "*   List item\n",
        "*   List item\n",
        "\n",
        "\n",
        "\n",
        "```python\n",
        "import requests\n",
        "```\n",
        "- **Purpose**: This line imports the `requests` library, which is used for making HTTP requests in Python. It allows you to send HTTP/1.1 requests with methods like `GET` and `POST`, enabling you to interact with web pages and APIs.\n",
        "\n",
        "```python\n",
        "from bs4 import BeautifulSoup\n",
        "```\n",
        "- **Purpose**: This line imports the `BeautifulSoup` class from the `bs4` module, which is part of the BeautifulSoup library. BeautifulSoup is a Python library used for parsing HTML and XML documents and extracting data from them.\n",
        "\n",
        "```python\n",
        "print(\"Fetching the webpage...\")\n",
        "```\n",
        "- **Purpose**: This line outputs the message \"Fetching the webpage...\" to the console, letting the user know that the script is about to request the webpage.\n",
        "\n",
        "```python\n",
        "url = \"https://news.ycombinator.com/\"\n",
        "```\n",
        "- **Purpose**: This line defines the `url` variable, storing the URL of the webpage you want to scrape. In this case, it's the Hacker News homepage.\n",
        "\n",
        "```python\n",
        "response = requests.get(url)\n",
        "```\n",
        "- **Purpose**: This line sends a GET request to the URL stored in the `url` variable using the `requests.get()` method. The response from the server (which includes the HTML content of the webpage) is stored in the `response` variable.\n",
        "\n",
        "```python\n",
        "if response.status_code == 200:\n",
        "```\n",
        "- **Purpose**: This line checks if the request was successful by evaluating the status code of the response. HTTP status code `200` means \"OK\", indicating that the webpage was successfully fetched.\n",
        "\n",
        "```python\n",
        "print(\"Webpage fetched successfully!\")\n",
        "```\n",
        "- **Purpose**: If the status code is `200`, this line prints a confirmation message to the console, indicating that the webpage was successfully fetched.\n",
        "\n",
        "```python\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "```\n",
        "- **Purpose**: This line creates a `BeautifulSoup` object named `soup` by parsing the HTML content of the webpage (available in `response.text`). The `'html.parser'` argument specifies that the built-in Python HTML parser should be used to parse the document. The `soup` object now represents the HTML document in a structured format, allowing for easy data extraction.\n",
        "\n",
        "```python\n",
        "print(\"Parsing the headlines...\")\n",
        "```\n",
        "- **Purpose**: This line outputs the message \"Parsing the headlines...\" to the console, letting the user know that the script is about to extract the headlines from the parsed HTML.\n",
        "\n",
        "```python\n",
        "headlines = soup.find_all('span', class_='titleline')\n",
        "```\n",
        "- **Purpose**: This line searches the parsed HTML (`soup`) for all `<span>` elements with the class `titleline`. The `find_all()` method returns a list of all matching elements, which is stored in the `headlines` variable. Each element in this list corresponds to a headline on the webpage.\n",
        "\n",
        "```python\n",
        "if headlines:\n",
        "```\n",
        "- **Purpose**: This line checks if the `headlines` list is not empty (i.e., it contains at least one element). If it is non-empty, the following block of code (inside the `if` statement) will be executed.\n",
        "\n",
        "```python\n",
        "for index, headline in enumerate(headlines):\n",
        "```\n",
        "- **Purpose**: This line starts a `for` loop that iterates over each element in the `headlines` list. The `enumerate()` function is used to keep track of the index of each element, starting from 0. The `index` variable stores the current index, and `headline` stores the current headline element.\n",
        "\n",
        "```python\n",
        "print(f\"{index + 1}. {headline.text}\")\n",
        "```\n",
        "- **Purpose**: Inside the loop, this line prints the index and the text of the current headline. The `f` before the string indicates an f-string, which allows you to embed expressions inside curly braces `{}` within the string. `index + 1` gives a 1-based index (starting from 1 instead of 0), and `headline.text` extracts the text content from the current `headline` element.\n",
        "\n",
        "```python\n",
        "else:\n",
        "    print(\"No headlines found.\")\n",
        "```\n",
        "- **Purpose**: If the `headlines` list is empty (i.e., no matching elements were found), this `else` block is executed, printing the message \"No headlines found.\"\n",
        "\n",
        "```python\n",
        "else:\n",
        "    print(f\"Failed to fetch the webpage. Status code: {response.status_code}\")\n",
        "```\n",
        "- **Purpose**: If the initial check `if response.status_code == 200` fails (meaning the status code is not 200), this `else` block is executed. It prints a message indicating that the webpage could not be fetched, along with the actual status code returned by the server.\n",
        "\n",
        "### Summary:\n",
        "- The code fetches the Hacker News homepage, checks if the request was successful, parses the HTML to find all headlines, and prints them. If something goes wrong (e.g., the request fails or no headlines are found), it outputs appropriate error messages."
      ],
      "metadata": {
        "id": "rkt6uGGYjEhp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#"
      ],
      "metadata": {
        "id": "8VaXeksqlKtF"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **IMAGE SCRAPPING**\n"
      ],
      "metadata": {
        "id": "9Glc3Wa2kurE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Step 1: Define the URL of the webpage to scrape\n",
        "url = \"https://unsplash.com/images/things/toys\"  # Replace with the URL you want to scrape\n",
        "\n",
        "# Step 2: Send a request to the website\n",
        "response = requests.get(url)\n",
        "\n",
        "# Step 3: Parse the HTML content\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# Step 4: Create a folder to save the images\n",
        "if not os.path.exists('downloaded_images'):\n",
        "    os.makedirs('downloaded_images')\n",
        "\n",
        "# Step 5: Find all image tags <img> in the HTML\n",
        "images = soup.find_all('img')\n",
        "\n",
        "# Step 6: Loop through the image tags and download the images\n",
        "for index, img in enumerate(images):\n",
        "    img_url = img.get('src')\n",
        "\n",
        "    # Some images might have relative URLs\n",
        "    if img_url and not img_url.startswith('http'):\n",
        "        img_url = url + img_url\n",
        "\n",
        "    if img_url:  # Proceed if img_url is valid\n",
        "        # Step 7: Send a request to the image URL\n",
        "        img_response = requests.get(img_url, stream=True)\n",
        "\n",
        "        # Step 8: Save the image to the folder with a unique name\n",
        "        img_path = os.path.join('downloaded_images', f'image_{index}.jpg')\n",
        "        with open(img_path, 'wb') as img_file:\n",
        "            shutil.copyfileobj(img_response.raw, img_file)\n",
        "        print(f\"Image {index} downloaded and saved as {img_path}\")\n",
        "\n",
        "# Step 9: Confirm completion\n",
        "print(\"All images downloaded and saved successfully!\")\n",
        "\n",
        "# Step 10: Create a readme.txt or description.txt file\n",
        "with open('downloaded_images/readme.txt', 'w') as readme_file:\n",
        "    readme_file.write(\"This folder contains images scraped from the following URL: \" + url + \"\\n\")\n",
        "    readme_file.write(\"Steps:\\n\")\n",
        "    readme_file.write(\"1. Define the URL of the webpage to scrape.\\n\")\n",
        "    readme_file.write(\"2. Send a request to the website to get the HTML content.\\n\")\n",
        "    readme_file.write(\"3. Parse the HTML content using BeautifulSoup.\\n\")\n",
        "    readme_file.write(\"4. Create a folder to save the images if it doesn't already exist.\\n\")\n",
        "    readme_file.write(\"5. Find all image tags <img> in the HTML content.\\n\")\n",
        "    readme_file.write(\"6. Loop through the image tags and download the images.\\n\")\n",
        "    readme_file.write(\"7. Send a request to each image URL and save the image.\\n\")\n",
        "    readme_file.write(\"8. Save each image to the folder with a unique name.\\n\")\n",
        "    readme_file.write(\"9. Confirm that all images are downloaded and saved successfully.\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dzhbprjbdH6J",
        "outputId": "6100a5e9-7b64-456d-9f94-ef2dfd7f52cc"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All images downloaded and saved successfully!\n"
          ]
        }
      ]
    }
  ]
}